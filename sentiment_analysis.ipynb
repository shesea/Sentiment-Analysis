{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contrary-kidney",
   "metadata": {},
   "source": [
    "# Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-anniversary",
   "metadata": {},
   "source": [
    "На основании коллекции размеченных коротких сообщений построить модель, предсказывающую общую тональность сообщения. Множество меток в данном случае - {позитивное, нейтральное, негативное}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-jonathan",
   "metadata": {},
   "source": [
    "# Методы решения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-afternoon",
   "metadata": {},
   "source": [
    "Задачи сентимент-анализа решаются с помощью следующих подходов:  \n",
    "    - подходы, основанные на правилах;  \n",
    "    - подходы, основанные на словарях;  \n",
    "    - машинное обучение с учителем;  \n",
    "    - машинное обучение без учителя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-banks",
   "metadata": {},
   "source": [
    "# NB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-philippines",
   "metadata": {},
   "source": [
    "- Классификация выполняется легко и быстро  \n",
    "- Хорошо работает с категорийными признаками  \n",
    "- Приходится делать допущение о независимости признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-james",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-advocacy",
   "metadata": {},
   "source": [
    "- Эффективен в случаях где набор измерений больше, чем обучающих данных\n",
    "- Эффективен при многомерных векторах\n",
    "- Небольшие изменения в обучающих данных не очень влияют на работу свм\n",
    "- Не подходит для огромных датасетов\n",
    "- Не очень справляется с шумом\n",
    "- Медленнее по времени"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-taxation",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus.reader import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-council",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iraqi-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, header):\n",
    "    marks_df = pd.read_csv(path, header=header)\n",
    "    return marks_df\n",
    "\n",
    "np.random.seed(500)\n",
    "\n",
    "data = pd.read_csv('train.txt', encoding='utf-8', delimiter = '\\t', names=['label', 'text'])\n",
    "test = pd.read_csv('test_1.txt', encoding='utf-8', delimiter = '\\t', names=['label', 'text'])\n",
    "    \n",
    "data['text'] = [re.sub(r'[^\\w\\s]', '', entry) for entry in data['text']]\n",
    "test['text'] = [re.sub(r'[^\\w\\s]', '', entry) for entry in test['text']]\n",
    "\n",
    "# Lower case\n",
    "data['text'] = [entry.lower() for entry in data['text']]\n",
    "test['text'] = [entry.lower() for entry in test['text']]\n",
    "\n",
    "# Tokenization\n",
    "data['text'] = [word_tokenize(entry) for entry in data['text']]\n",
    "test['text'] = [word_tokenize(entry) for entry in test['text']]\n",
    "\n",
    "# Removing Stop Words\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "russian_stopwords -= {'хорошо', 'ничего', 'нет', 'не', 'никогда', 'нельзя', 'ни', 'лучше', 'другой', 'впрочем', 'опять', 'более'}\n",
    "for i, s in enumerate(data['text']):\n",
    "    data['text'][i] =  [entry for entry in data['text'][i] if entry not in russian_stopwords]\n",
    "for i, s in enumerate(test['text']):\n",
    "    test['text'][i] =  [entry for entry in test['text'][i] if entry not in russian_stopwords]\n",
    "\n",
    "# Lemmatizing\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    res = list()\n",
    "    for word in text:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "    return res\n",
    "\n",
    "data['text'] = [lemmatize(entry) for entry in data['text']]\n",
    "test['text'] = [lemmatize(entry) for entry in test['text']]\n",
    "\n",
    "Train_X, Train_Y, Test_X, Test_Y = [' '.join(s) for s in data['text']], data['label'], [' '.join(s) for s in test['text']], test['label']\n",
    "encoder = {'__label__neg': 0, '__label__neut': 1, '__label__pos': 2}\n",
    "decoder = ['__label__neg', '__label__neut', '__label__pos']\n",
    "for i, label in enumerate(Train_Y):\n",
    "    Train_Y[i] = encoder[label]\n",
    "for i, label in enumerate(Test_Y):\n",
    "    Test_Y[i] = encoder[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-sunday",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "awful-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "to_fit = []\n",
    "for s in data['text']:\n",
    "    to_fit.append(' '.join(s))\n",
    "for s in test['text']:\n",
    "    to_fit.append(' '.join(s))\n",
    "tfidf.fit(to_fit)\n",
    "Train_X_tfidf = tfidf.transform(Train_X)\n",
    "Test_X_tfidf = tfidf.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-hydrogen",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "normal-brazilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 0 2 1 0 0 0 1 0 2 0 2 1 1 0 1 2 1 2 0 0 1 0 0 2 2 1 2 1 1 1 1 1 2 2\n",
      " 0 1 2 2 0 2 2 1 2 2 0 0 2 1 1 1 1 2 2 0 2 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
      " 2 0 0 2 1 2 1 1 1 0 1 0 1 1 1 1 1 1 1 2 2 1 2 1 0 0 0 1 0 0 1 1 1 1 1 1 0\n",
      " 0 1 1 0 0 2 0 0 1 0 1 0 2 1 1 0 0 1 0 1 0 1 1 1 2 0 0 1 0 0 0 2 1 2 2 1 1\n",
      " 0 1 2 1 2 2 0 2 0 0 0 1 1 1 1 1 1 1 0 1 2 0 0 2 0 0 0 1 1 2 0 1 1 1 0 1 2\n",
      " 0 1 0 2 0 0 2 2 0 0 0 0 2 2 0]\n",
      "SVM Accuracy Score ->  76.2582056892779\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "Train_Y = Train_Y.astype('int')\n",
    "SVM.fit(Train_X_tfidf, Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_tfidf)\n",
    "Test_Y = Test_Y.astype('int')\n",
    "print(predictions_SVM[:200])\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-carolina",
   "metadata": {},
   "source": [
    "# NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "macro-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  74.9890590809628\n"
     ]
    }
   ],
   "source": [
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_tfidf,Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X_tfidf)\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "architectural-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.756     0.799     0.777      1614\n",
      "           1      0.728     0.741     0.735      1546\n",
      "           2      0.768     0.703     0.734      1410\n",
      "\n",
      "    accuracy                          0.750      4570\n",
      "   macro avg      0.751     0.748     0.749      4570\n",
      "weighted avg      0.750     0.750     0.749      4570\n",
      "\n",
      "SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.763     0.803     0.782      1614\n",
      "           1      0.728     0.766     0.746      1546\n",
      "           2      0.808     0.713     0.757      1410\n",
      "\n",
      "    accuracy                          0.763      4570\n",
      "   macro avg      0.766     0.761     0.762      4570\n",
      "weighted avg      0.765     0.763     0.762      4570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(Test_Y, predictions_NB, digits=3)\n",
    "print('NB\\n', report)\n",
    "report = classification_report(Test_Y, predictions_SVM, digits=3)\n",
    "print('SVM\\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "industrial-junior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__neg Мдааа а что вы еще ожидали от мошенников\n",
      "__label__pos Люблю Россию сильной любовью..........\n",
      "__label__neg я не счастлив и мне это не нравится!\n",
      "__label__neg Путин любит быдло\n",
      "__label__neg никому порекомендовать!!! этот БРЕД не могу\n",
      "__label__neg Крым\n",
      "__label__pos птички поют, на душе хорошо!\n",
      "__label__pos этот фильм ужасов супер, противный как надо\n",
      "__label__pos я поспала здоровым 4-часовым сном\n",
      "__label__neut Поскольку ТКС Банк работает со своими клиентами исключительно через дистанционные каналы, центр обслуживания клиентов является одним из основных каналов коммуникации с клиентами.\n",
      "__label__pos Зенит выиграл у Спартака\n",
      "__label__neut Москва\n",
      "__label__neut Комсомольск-на-Амуре\n",
      "__label__neut Санкт-Петербург\n"
     ]
    }
   ],
   "source": [
    "fin = codecs.open('check_tweets.txt', 'r', 'utf-8')\n",
    "tweets_to_predict = fin.readlines()\n",
    "tweets_to_predict = [tweet.rstrip() for tweet in tweets_to_predict]\n",
    "started_tweets = tweets_to_predict\n",
    "\n",
    "tweets_to_predict = [re.sub(r'[^\\w\\s]', '', entry) for entry in tweets_to_predict]\n",
    "# Lower case\n",
    "tweets_to_predict = [entry.lower() for entry in tweets_to_predict]\n",
    "# Tokenization\n",
    "tweets_to_predict = [word_tokenize(entry) for entry in tweets_to_predict]\n",
    "# Removing Stop Words\n",
    "for i, s in enumerate(tweets_to_predict):\n",
    "    tweets_to_predict[i] =  [entry for entry in tweets_to_predict[i] if entry not in russian_stopwords]\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    res = list()\n",
    "    for word in text:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "    return res\n",
    "\n",
    "tweets_to_predict = [lemmatize(entry) for entry in tweets_to_predict]\n",
    "\n",
    "tweets_to_predict = [' '.join(s) for s in tweets_to_predict]\n",
    "\n",
    "vectorized_tweets = tfidf.transform(tweets_to_predict)\n",
    "\n",
    "predicted = SVM.predict(vectorized_tweets)\n",
    "for i, label in enumerate(predicted):\n",
    "    print(decoder[label], started_tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-manhattan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
